# ğŸ§  AmbedkarGPT â€” RAG-Based Q&A System  
### _AI Intern Hiring Assignment â€” Kalpit Pvt Ltd, UK_

This repository contains my solution for the **AI Intern Hiring â€“ Phase 1: Core Skills Evaluation**.  
The system loads a short speech by **Dr. B.R. Ambedkar**, converts it into embeddings, retrieves relevant chunks for a question, and generates an answer using a local LLM â€” *all offline & free*.

---

## ğŸ“ **Project Structure**

```
project/
â”‚â”€â”€ assignment/                 
â”‚     â”‚â”€â”€ chroma_db/           
â”‚     â”‚â”€â”€ text_files/           
â”‚     â”‚     â””â”€â”€ speech.txt     
â”‚     â”‚â”€â”€ rag_ass.ipynb         
â”‚
â”‚â”€â”€ requirements.txt           
â”‚â”€â”€ README.md                   


## ğŸ› ï¸ **Tech Stack**

| Component        | Library/Tool |
|------------------|--------------|
| Programming Lang | Python 3.8+ |
| Framework        | LangChain |
| Vector DB        | ChromaDB |
| Embeddings       | HuggingFace MiniLM |
| LLM              | Ollama (Mistral 7B) |
| Notebook         | Jupyter Notebook |

---

## ğŸ“¦ **Installation & Setup**

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/<your-username>/AmbedkarGPT-Intern-Task.git
cd AmbedkarGPT-Intern-Task
```

---

### 2ï¸âƒ£ Create Virtual Environment
```bash
python -m venv venv
```

#### **Windows**
```bash
venv\Scripts\activate
```

#### **Mac/Linux**
```bash
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ¤– **Install Ollama (Required)**

Install Ollama:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

Pull Mistral:
```bash
ollama pull mistral
```

---

## â–¶ï¸ **Running the Project**

### **Run via Jupyter Notebook**
```bash
jupyter notebook rag_ass.ipynb
```

---

## ğŸ§  **How the RAG Pipeline Works**

1. **Load Data**  
   Loads the text file using `TextLoader`.

2. **Text Splitting**  
   Splits the speech into chunks using `RecursiveCharacterTextSplitter`.

3. **Embedding Creation**  
   Generates embeddings using  
   **sentence-transformers/all-MiniLM-L6-v2**

4. **Vector Store (ChromaDB)**  
   Stores embeddings locally for retrieval.

5. **Retrieval**  
   Finds the most relevant chunks based on the user question.

6. **LLM Response Generation**  
   Sends question + retrieved chunks to **Ollama Mistral**.

7. **Final Answer**  
   A clean natural-language answer is returned.

---

## ğŸ“„ **Example Query**

```
Question:
"What does Ambedkar say about caste and shastras?"

Answer:
(Generated by Mistral using retrieved context)
```

---

## ğŸ“š **Dataset**

The speech used is a short excerpt from **Annihilation of Caste** by Dr. B.R. Ambedkar.  
Included inside: `text_files/speech.txt`


## ğŸ‘¤ **Author**

**Abdul Ahad Rauf**  


---
